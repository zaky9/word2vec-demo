Word embedding is a technique to represent words into vectors in a predefined vector space. Every word is assigned to a single vector and the vector values are learned in a manner like a neural network. Therefore, The method is frequently referred to as deep learning. One of the most common method is **word2vec** developed by [Mikolov T. et al. (2013) ](https://arxiv.org/pdf/1310.4546.pdf).

word2vec uses Feed Forward Neural Network to store the embedding in the weight of neural netwroks. it need to find the weight of the NN, which effectively are the word embedding themselves. Given an input word, it predict whhether an output word appears in its context. FOr instance, given a sentence of "The quick brown fox jumps over the lazy dogs" and the input word is **"jumps"**, the correct output (in binary) should be words that are close to the word **jumps**. In the process of predicting the target word, we learn the vector representation of the target word.
